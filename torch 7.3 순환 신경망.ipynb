{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4411ec",
   "metadata": {},
   "source": [
    "torchtext는 자연어 처리 분야에서 사용하는 데이터로더이다.파일 가져오기, 토큰화, 단어 집합 생성, 인코딩, 단어 벡터 생성 등의 작업을 지원하기 때문에 자연어 처리에서 많이 사용되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11bd6179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95f3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.9 in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (0.9.0)\n",
      "Collecting torch==1.8.0\n",
      "  Using cached torch-1.8.0-cp38-cp38-manylinux1_x86_64.whl (735.5 MB)\n",
      "Requirement already satisfied: numpy in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from torchtext==0.9) (1.19.5)\n",
      "Requirement already satisfied: requests in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from torchtext==0.9) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from torchtext==0.9) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from torch==1.8.0->torchtext==0.9) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from requests->torchtext==0.9) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from requests->torchtext==0.9) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from requests->torchtext==0.9) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages (from requests->torchtext==0.9) (2021.10.8)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0+cu113\n",
      "    Uninstalling torch-1.11.0+cu113:\n",
      "      Successfully uninstalled torch-1.11.0+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
      "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.8.0 which is incompatible.\n",
      "pytorch-lightning 1.7.6 requires torch>=1.9.*, but you have torch 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.8.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/data/ydkim/.pyenv/versions/3.8.12/envs/py3.8.12/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfd95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext # 0.9\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f513b",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8815cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d84d2",
   "metadata": {},
   "source": [
    "* torchtext.legacy.data에서 제공하는 Field는 데이터 전처리를 위해 사용되며 여기에서 사용되는 파라미터는 다음과 같다.\n",
    "    1. lower: 대문자를 모두 소문자로 변경한다. 기본값은 false\n",
    "    2. fix_length: 고정된 길이의 데이터를 얻을 수 있다. 여기에서는 데이터의 길이를 200으로 고정했으며 200보다 짧다면 패딩을 시켜준다.\n",
    "    3. batch_first: 신경망에 입력되는 텐서의 첫 번째 차원 값이 배치 크기가 되도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094d59e",
   "metadata": {},
   "source": [
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32f4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b2190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['for', 'the', 'life', 'of', 'me,', 'why', 'did', 'this', 'film', 'receive', 'an', 'r', 'rating?!', 'while', 'it', 'is', 'about', 'flesh-eating', 'zombies,', 'believe', 'it', 'or', 'not,', \"it's\", 'actually', 'a', 'pretty', 'good', 'family-friendly', 'film--at', 'least', 'if', 'your', 'kids', 'are', 'age', '10', 'and', 'older.', 'unlike', 'the', 'traditional', 'zombie', 'films,', 'this', 'one', 'has', 'an', 'excellent', 'sense', 'of', 'humor', 'as', 'well', 'as', 'a', 'traditional', 'values--albeit', 'a', 'bit', 'twisted!', 'the', 'language', \"isn't\", 'a', 'serious', 'problem,', 'there', 'is', 'no', 'nudity', 'and', 'the', 'film', 'style', 'is', 'definitely', 'geared', 'towards', 'kids', '(much', 'like', 'the', 'old', 'tv', 'show', '\"eerie,', 'indiana\")--yet', 'some', 'knucklehead', 'slapped', 'an', 'r', 'rating', 'one', 'it!', 'believe', 'me,', 'most', 'kids', 'have', 'seen', 'worse', 'violence', 'than', 'this', 'and', 'it', 'just', 'seems', 'silly', 'to', 'make', 'audiences', 'think', 'this', 'is', 'an', 'adults', 'only', 'film.<br', '/><br', '/>the', 'story', 'is', 'set', 'in', 'a', 'parallel-type', 'world.', 'while', 'the', 'fashions,', 'cars', 'and', 'mores', 'appear', 'circa', '1953,', 'in', 'this', 'bizarro', 'world', 'there', 'has', 'been', 'a', 'fierce', 'recent', 'zombie', 'plague', 'that', 'resulted', 'in', 'the', '\"zombie', 'war\"', 'and', 'massive', 'changes', 'in', 'everyday', 'life.', 'at', 'school,', 'kids', 'are', 'trained', 'in', 'armed', 'combat', 'and', \"there's\", 'a', 'cute', 'scene', 'late', 'in', 'the', 'film', 'where', 'the', 'father', 'gives', 'his', 'son', 'a', 'handgun', 'and', 'tells', 'him', 'to', 'keep', 'it', 'in', 'his', 'backpack', '\"just', 'in', 'case\"!', 'as', 'for', 'life', 'outside', 'of', 'school,', \"it's\", 'pretty', 'weird', 'as', 'well,', 'as', 'people', 'now', 'have', 'learned', 'that', 'zombies', \"aren't\", 'such', 'a', 'bad', 'thing!', 'heck,', 'using', 'shock', 'collars', 'and', 'training,', 'they', 'can', 'be', 'made', 'into', 'slaves', 'who', 'can', 'do', 'your', 'housework,', 'clean', 'streets,', 'deliver', 'milk', 'or,', 'in', 'the', 'case', 'of', 'a', 'really', 'sick', 'guy,', 'be', 'your', '\"special', 'friend\".<br', '/><br', '/>this', 'film', 'deals', 'with', 'one', 'particular', 'family', 'that', 'finally', 'buys', 'their', 'first', 'zombie', 'slave', '(played', 'by', 'billy', 'connally).', 'mom', 'is', 'thrilled', 'and', 'her', 'son', 'slowly', 'becomes', 'the', \"zombie's\", 'friend.', 'dad,', 'on', 'the', 'other', 'hand,', \"isn't\", 'convinced--as', 'he', 'was', 'forced', 'years', 'early', 'to', 'kill', 'his', 'own', 'zombie', 'father', 'and', 'he', \"hasn't\", 'yet', 'gotten', 'over', 'this!!', 'funny,', 'irreverent', 'and', 'unique--this', 'film', 'needs', 'to', 'be', 'seen', 'by', 'a', 'much', 'wider', 'audience.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])) # 데이터셋의 내용을 보고자 할 때는 examples를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46414f8",
   "metadata": {},
   "source": [
    "## 데이터셋 전처리 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09586e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "for example in train_data.examples:\n",
    "    text = [x.lower() for x in vars(example)['text']] # 소문자로 변경\n",
    "    text = [x.replace('<br', '') for x in text] # '<br'을 ''으로 변경\n",
    "    text = [''.join(c for c in s if c not in string.punctuation) for s in text] # 구두점 제거\n",
    "    text = [s for s in text if s] # 공백 제거\n",
    "    vars(example)['text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762fb18",
   "metadata": {},
   "source": [
    "## 훈련과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8c9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(0), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4838f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 20000\n",
      "Number of validation examples 5000\n",
      "Number of testing examples 25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples', len(train_data))\n",
    "print('Number of validation examples', len(valid_data))\n",
    "print('Number of testing examples', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd35cf",
   "metadata": {},
   "source": [
    "## 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483fa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b436ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens is TEXT covab:  10002\n",
      "Unique tokens is LABEL covab:  3\n"
     ]
    }
   ],
   "source": [
    "print('Unique tokens is TEXT covab: ', len(TEXT.vocab))\n",
    "print('Unique tokens is LABEL covab: ', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bea4a1",
   "metadata": {},
   "source": [
    "* max_size : 단어 집합의 크기로 단어 집합에 포함되는 어휘 수를 의미한다.\n",
    "* min_freq : 훈련 데이터셋에서 특정 단어의 최소 등장 횟수를 의미한다.\n",
    "* vectors : 임베딩 벡터를 지정할 수 있다. 임베딩 벡터는 워드 임베딩의 결과로 나온 벡터이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71744cf",
   "metadata": {},
   "source": [
    "### 테스트 데이터셋의 단어 집합 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6c037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fdae053b760>>, {'<unk>': 0, 'pos': 1, 'neg': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e2582",
   "metadata": {},
   "source": [
    "## 데이터셋 메모리로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe2c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_dim = 100 # 각 단어를 100차원으로 조정\n",
    "hidden_size = 300\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "                                                (train_data, valid_data, test_data),\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd54f0",
   "metadata": {},
   "source": [
    "* BucketIterator는 데이터로더와 쓰임새가 같다. 즉, 배치 크기 단위로 값을 차례대로 꺼내어 메모리로 가져오고 싶을 때 사용한다. 특히 Field에서 fix_length를 사용하지 않았다면 BucketIterator는 비슷한 길이의 데이터를 한 배치에 할당하여 패딩을 최소화시켜 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cf73d",
   "metadata": {},
   "source": [
    "## 워드 임베딩 및 RNN 셀 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(RNNCell_Encoder, self).__init__()\n",
    "        self.rnn = nn.RNNCell(input_dim, hidden_size) # ⓵\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bz = inputs.shape[1] # 배치\n",
    "        ht = torch.zeros((bz, hidden_size)).to(device) # 배치와 은닉층 뉴런의 크기를 0으로 초기화\n",
    "        for word in inputs:\n",
    "            ht = self.rnn(word, ht) # ⓶\n",
    "            \n",
    "        return ht\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(len(TEXT.vocab.stoi), embedding_dim) # ⓷\n",
    "        self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        x = self.rnn(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf483b7e",
   "metadata": {},
   "source": [
    "⓶ : 재귀적으로 발생하는 상태 값을 처리하기 위한 구문이다.  \n",
    "* ht : 현재 상태를 의미하는 것  \n",
    "* word : 현재의 입력 벡터로 (Batch, input_size)형태를 갖는다.  \n",
    "* ht : 이전 상태를 의미하는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0197f9b",
   "metadata": {},
   "source": [
    "## 옵티마이저와 손실 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ea7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9348d3",
   "metadata": {},
   "source": [
    "## 모델 학습을 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee37b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, trainloader, validloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for b in trainloader:\n",
    "        x, y = b.text, b.label # trainloader에서 text와 label을 꺼내온다.\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    valid_running_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b in validloader:\n",
    "            x, y = b.text, b.label\n",
    "            x, y= x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            valid_correct += (y_pred == y).sum().item()\n",
    "            valid_total += y.size(0)\n",
    "            valid_running_loss += loss.item()\n",
    "            \n",
    "    epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
    "    epoch_valid_acc = valid_correct / valid_total\n",
    "    \n",
    "    print('epoch: ', epoch,\n",
    "          'loss: ', round(epoch_loss, 3),\n",
    "          'accuracy: ', round(epoch_acc, 3),\n",
    "          'valid_loss: ', round(epoch_valid_loss, 3),\n",
    "          'valid_acc: ', round(epoch_valid_acc, 3)\n",
    "         )\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a56330",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbcc3d92",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42215/95209405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(\n\u001b[0m\u001b[1;32m      9\u001b[0m                                                                 epoch, model, train_iterator, valid_iterator)\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 훈련 데이텃세을 모델에 적용했을 때의 오차\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42215/3488778014.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, model, trainloader, validloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;31m# trainloader에서 text와 label을 꺼내온다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42215/2733379802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42215/2733379802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 배치와 은닉층 뉴런의 크기를 0으로 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ⓶\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/py3.8.12/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             ret = _VF.rnn_tanh_cell(\n\u001b[0m\u001b[1;32m    965\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(\n",
    "                                                                epoch, model, train_iterator, valid_iterator)\n",
    "    train_loss.append(epoch_loss) # 훈련 데이텃세을 모델에 적용했을 때의 오차\n",
    "    train_acc.append(epoch_acc) # 훈련 데이터셋을 모델에 적용했을 때의 정확도\n",
    "    valid_loss.append(epoch_valid_loss) # 검증 데이터셋을 모델에 적용했을 때의 오차\n",
    "    valid_acc.append(epoch_valid_acc) # 검증 데이터셋을 모델에 적용했을 때의 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b4f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
